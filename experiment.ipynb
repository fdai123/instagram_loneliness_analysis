{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xlrd\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#customize data reading here\n",
    "pth = \"/UC San Diego/Sherry Jiang - Instagram Study Subject Data/\"\n",
    "pth2 = \"/UC\\ San\\ Diego/Sherry\\ Jiang\\ -\\ Instagram\\ Study\\ Subject\\ Data/\"\n",
    "# bash command: ls -d */ | cut -f1 -d'/' > directories.txt\n",
    "#sheet = xlrd.open_workbook(pth + 'quatrics_data.xlsx').sheet_by_name('Social+Media+and+Health_January')\n",
    "#profile_id = [[sheet.cell_value(r, c) for c in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "\n",
    "with open('directories.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "checked=[]\n",
    "\"\"\"for i in profile_id:\n",
    "    if i not in lines:\n",
    "        checked.append(i)\n",
    "print(checked)\"\"\"\n",
    "# captions, nltk -> polarity \n",
    "# image data -> hue, saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "from pathlib import Path\n",
    "from skimage import color, io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(img):\n",
    "    clf = KMeans(n_clusters = 5)\n",
    "    color_labels = clf.fit_predict(img)\n",
    "    center_colors = clf.cluster_centers_\n",
    "    counts = Counter(color_labels)\n",
    "    ordered_colors = [center_colors[i] for i in counts.keys()]\n",
    "    hex_colors = [rgb_to_hex(ordered_colors[i]) for i in counts.keys()]\n",
    "\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.pie(counts.values(), labels = hex_colors, colors = hex_colors)\n",
    "\n",
    "    plt.savefig(\"results/my_pie.png\")\n",
    "    print(\"Found the following colors:\\n\")\n",
    "    for color in hex_colors:\n",
    "      print(color)\n",
    "\n",
    "def rgb_to_hex(rgb_color):\n",
    "    hex_color = \"#\"\n",
    "    for i in rgb_color:\n",
    "        hex_color += (\"{:02x}\".format(int(i)))\n",
    "    return hex_color\n",
    "\n",
    "def preprocess(raw):\n",
    "    image = cv2.resize(raw, (900, 600), interpolation = cv2.INTER_AREA)                                          \n",
    "    image = image.reshape(image.shape[0]*image.shape[1], 3)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  get_hsv: converts image from rgb to hsv (Hue, Saturation, V=Brightness)\n",
    "def get_hsv(path):\n",
    "\tif not os.path.exists(Path(path)):\n",
    "\t\treturn 0\n",
    "\tim = io.imread(path)\n",
    "\tim = color.rgb2hsv(im) # http://scikit-image.org/docs/dev/api/skimage.color.html#rgb2hsv\n",
    "\thsv = []\n",
    "\tfor i in range(len(im.shape)):\n",
    "\t\tif len(im.shape) == 2: #b/w photo\n",
    "\t\t\timdata = im[:,i]\n",
    "\t\telif len(im.shape) == 3: #color photo\n",
    "\t\t\timdata = im[:,:,i]\n",
    "\t\tavg = np.mean(imdata)\n",
    "\t\thsv.append( avg )\n",
    "\treturn hsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = []\n",
    "num_img, num_posts = 0, 0\n",
    "for i in lines:\n",
    "    # emojis -> convert into more meaningful words?\n",
    "    # loop through all profiles and get the relevant json file\n",
    "    #p_img = pth2 + 'instagram_data/'+i[:-1]+\"/media/posts/\"\n",
    "    p_img = pth + \"instagram_data/\" + i[:-1] + \"/\"\n",
    "    p_text = pth + \"instagram_data/\" + i[:-1] + \"/content/posts_1.json\"\n",
    "    if not os.path.exists(Path(p_text)):\n",
    "        continue\n",
    "    #f = open(p_text)\n",
    "    with open(p_text, 'r') as f:\n",
    "        content = f.read()\n",
    "        print(i)\n",
    "        data = json.loads(content)\n",
    "    for d in data:\n",
    "        if not isinstance(d, dict) or 'title' not in d.keys():\n",
    "            continue\n",
    "        title = d['title'].encode('latin1').decode('utf8')\n",
    "        \n",
    "        ps = sia.polarity_scores(title)\n",
    "        hsv = [0, 0, 0]\n",
    "        count = 0\n",
    "        num_posts += 1\n",
    "        for m in d['media']:\n",
    "            if \".jpg\" in m[\"uri\"]:\n",
    "                thsv = get_hsv(p_img + m[\"uri\"])\n",
    "                if thsv == 0:\n",
    "                    continue\n",
    "                hsv = [a + b for a, b in zip(hsv, thsv)]\n",
    "                count += 1\n",
    "                num_img += 1\n",
    "        if count != 0:\n",
    "            avg_hsv = [x / count for x in hsv]\n",
    "        saved_data.append([i[:-1], d[\"creation_timestamp\"], title, ps, avg_hsv[0], avg_hsv[1], avg_hsv[2]])\n",
    "        # creation timestamp | text | polarity score | profile-id -> csv\n",
    "        # hsv extraction -> output compare with current\n",
    "        # analysis writeup -> methods used in analysis/feature extraction(text: captions, vader polarity score, number of files; image: hsv, number of images, normalization/mean)\n",
    "    \"\"\"cmd = \"cd \"+p_img + \"; ls -d */ | cut -f1\"\n",
    "    dates = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n",
    "    dates = dates.communicate()[0].strip().decode(\"utf-8\").split(\"\\n\")\n",
    "    p_img = pth + 'instagram_data/'+i[:-1]+\"/media/posts/\"\n",
    "    for d in dates:\n",
    "        file_list=os.listdir(p_img+d[:-1])\n",
    "        for f in file_list:\n",
    "            hsv = get_hsv(p_img+d[:-1]+\"/\"+f)\n",
    "            print(hsv)\n",
    "            #image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            #modified_image = preprocess(image)\n",
    "            #analyze(modified_image)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fields = ['id', 'timestamp', 'caption', 'polarity score', 'hue', 'saturation', 'brightness']\n",
    "with open(\"./instagram_posts_data.csv\", 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow(fields)\n",
    "    write.writerows(saved_data)\n",
    "print(num_img, num_posts)\n",
    "# print(captions[3].encode('latin1').decode('utf8'))\n",
    "# cleaning: english language only; \n",
    "# image stats (hsv) -> logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "x = pd.read_csv(\"./instagram_posts_data.csv\")\n",
    "survey = pd.read_csv(\"./survey_with_scores.csv\")\n",
    "print(x.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = survey[['ResponseId', 'UCLA_LS_mean','UCLA_lonely']]\n",
    "color_X = x[['id', 'hue', 'saturation', 'brightness']]\n",
    "color_X = color_X.groupby(['id'])['hue', 'saturation', 'brightness'].mean().reset_index()\n",
    "color_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = y.ResponseId.isin(color_X.id)\n",
    "y = y[m]\n",
    "y = y.sort_values('ResponseId').reset_index()\n",
    "color_X = color_X.sort_values('id').reset_index()\n",
    "print(color_X.head())\n",
    "print(y.head())\n",
    "# most recent 3/6/12 months' posts -> number of posts -> number of pictures\n",
    "# binary/multiclass/linear classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_X = color_X.drop(index=[60], axis=0).reset_index()\n",
    "color_X['UCLA_LS_mean'] = y['UCLA_LS_mean'].to_numpy()\n",
    "color_X['UCLA_lonely'] = y['UCLA_lonely'].to_numpy()\n",
    "print(color_X.id, y.ResponseId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_X = color_X.dropna()\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(color_X[['hue', 'saturation', 'brightness']].values, color_X[['UCLA_lonely']].values, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression().fit(x_train, y_train)\n",
    "pred = clf.predict(x_test)\n",
    "score = clf.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = []\n",
    "num_comments, num_posts, num_following, num_follower = [], [], [], []\n",
    "for i in lines:\n",
    "    # emojis -> convert into more meaningful words?\n",
    "    # loop through all profiles and get the relevant json file\n",
    "    #p_img = pth2 + 'instagram_data/'+i[:-1]+\"/media/posts/\"\n",
    "    p_comment = pth + \"instagram_data/\" + i[:-1] + \"/comments/post_comments.json\"\n",
    "    p_profilephoto = pth + \"instagram_data/\" + i[:-1] + \"/content/profile_photos.json\"\n",
    "    p_search = pth + \"instagram_data/\" + i[:-1] + \"/recent_searches/word_or_phrases_searches.json\"\n",
    "    p_likedposts = pth + \"instagram_data/\" + i[:-1] + \"/likes/liked_posts.json\"\n",
    "    p_likedcomments = pth + \"instagram_data/\" + i[:-1] + \"/likes/liked_comments.json\"\n",
    "    p_followers = pth + \"instagram_data/\" + i[:-1] + \"/followers_and_following/followers.json\"\n",
    "    p_following = pth + \"instagram_data/\" + i[:-1] + \"/followers_and_following/following.json\"\n",
    "    p_priv = pth + \"instagram_data/\" + i[:-1] + \"/login_and_account_creation/account_privacy_changes.json\"\n",
    "\n",
    "    if not os.path.exists(Path(p_profilephoto)):\n",
    "        continue\n",
    "    \n",
    "    comments = []\n",
    "    with open(p_comment, 'r') as f:\n",
    "        content = f.read()\n",
    "        print(i)\n",
    "        data = json.loads(content)\n",
    "    for d in data:\n",
    "        if not isinstance(d, dict) or 'title' not in d.keys():\n",
    "            continue\n",
    "        title = d['title'].encode('latin1').decode('utf8')\n",
    "        value = d['string_list_data'][0]['value'].encode('latin1').decode('utf8')\n",
    "        \n",
    "        #title = d['value'].encode('latin1').decode('utf8')\n",
    "        l = [i, title, value, d['string_list_data'][0]['timestamp']]\n",
    "        comments.append(l)\n",
    "    \n",
    "    searches = []\n",
    "    with open(p_search, 'r') as f:\n",
    "        content = f.read()\n",
    "        data = json.loads(content)\n",
    "    for d in data:\n",
    "        if not isinstance(d, dict) or 'string_list_data' not in d.keys():\n",
    "            continue\n",
    "        l = [i, d['string_list_data'][0]['value'], d['string_list_data'][0]['timestamp']]\n",
    "        searches.append(l)\n",
    "\n",
    "    with open(p_likedposts, 'r') as f:\n",
    "        content = f.read()\n",
    "        data = json.loads(content)\n",
    "    num_posts.append(len(data))\n",
    "\n",
    "\n",
    "    with open(p_likedcomments, 'r') as f:\n",
    "        content = f.read()\n",
    "        data = json.loads(content)\n",
    "    num_comments.append(len(data))\n",
    "\n",
    "    with open(p_followers, 'r') as f:\n",
    "        content = f.read()\n",
    "        data = json.loads(content)\n",
    "    num_follower.append(len(data))\n",
    "\n",
    "\n",
    "    with open(p_following, 'r') as f:\n",
    "        content = f.read()\n",
    "        data = json.loads(content)\n",
    "    num_following.append(len(data))\n",
    "\n",
    "    privacy = []\n",
    "    with open(p_priv, 'r') as f:\n",
    "        content = f.read()\n",
    "        data = json.loads(content)\n",
    "    for d in data:\n",
    "        if not isinstance(d, dict) or 'title' not in d.keys():\n",
    "            continue\n",
    "        l = [i, d['title'], d['string_list_data'][0]['timestamp']]\n",
    "        privacy.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = []\n",
    "num_comments, num_posts, num_following, num_follower = [], [], [], []\n",
    "\n",
    "for i in lines:\n",
    "    p_comment = pth + \"instagram_data/\" + i[:-1] + \"/comments/post_comments.json\"\n",
    "\n",
    "    if not os.path.exists(Path(p_comment)):\n",
    "        continue\n",
    "    \n",
    "    comments = []\n",
    "    with open(p_comment) as f:\n",
    "        print(i)\n",
    "        data = json.load(f)\n",
    "    #f = open(p_comment)\n",
    "    #print(type(f))\n",
    "    #data = json.load(f)\n",
    "    \n",
    "    for d in data:\n",
    "        print(d)\n",
    "        if not isinstance(d, dict) or 'title' not in d.keys():\n",
    "            continue\n",
    "        #title = d['value'].encode('latin1').decode('utf8')\n",
    "        print('jdiosjfai')\n",
    "        l = [i, d['title'], d['string_list_data'][0]['value'], d['string_list_data'][0]['timestamp']]\n",
    "        comments.append(l)\n",
    "    \n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = []\n",
    "num_comments, num_posts, num_following, num_follower = [], [], [], []\n",
    "comments = []\n",
    "for i in lines:\n",
    "    p_comment = pth + \"instagram_data/\" + i[:-1] + \"/comments/post_comments.json\"\n",
    "\n",
    "    if not os.path.exists(Path(p_comment)):\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    with open(p_comment, 'r') as f:\n",
    "        print(i)\n",
    "        data = json.loads(f.read())\n",
    "    data = pd.json_normalize(data, record_path =['comments_media_comments'])\n",
    "\n",
    "    for ind, d in data.iterrows():\n",
    "        #if not isinstance(d, dict) or 'title' not in d.keys():\n",
    "        #    continue\n",
    "\n",
    "        title = d['title'].encode('latin1').decode('utf8')\n",
    "        value = d['string_list_data'][0]['value'].encode('latin1').decode('utf8')\n",
    "\n",
    "        l = [i[:-1], title, value, d['string_list_data'][0]['timestamp']]\n",
    "        comments.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fields = ['id', 'commenter_username', 'comment', 'timestamp']\n",
    "with open(\"./instagram_comments_data.csv\", 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow(fields)\n",
    "    write.writerows(comments)\n",
    "print(len(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = []\n",
    "num_comments, num_posts, num_following, num_follower = [], [], [], []\n",
    "searches = []\n",
    "for i in lines:\n",
    "    p_search = pth + \"instagram_data/\" + i[:-1] + \"/recent_searches/word_or_phrase_searches.json\"\n",
    "\n",
    "    if not os.path.exists(Path(p_search)):\n",
    "        continue\n",
    "    print(i)\n",
    "    \n",
    "    with open(p_search, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    data = pd.json_normalize(data, record_path =['searches_keyword'])\n",
    "    \n",
    "    print(data)\n",
    "    for ind, d in data.iterrows():\n",
    "            #if not isinstance(d, dict) or 'string_list_data' not in d.keys():\n",
    "            #    continue\n",
    "        l = [i[:-1], d['string_map_data.Search.value'], d['string_map_data.Time.timestamp']]\n",
    "        searches.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['id', 'search_value', 'timestamp']\n",
    "with open(\"./instagram_searches_data.csv\", 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow(fields)\n",
    "    write.writerows(searches)\n",
    "print(len(searches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = []\n",
    "for i in lines:\n",
    "\n",
    "    p_likedposts = pth + \"instagram_data/\" + i[:-1] + \"/likes/liked_posts.json\"\n",
    "    p_likedcomments = pth + \"instagram_data/\" + i[:-1] + \"/likes/liked_comments.json\"\n",
    "    p_followers = pth + \"instagram_data/\" + i[:-1] + \"/followers_and_following/followers.json\"\n",
    "    p_following = pth + \"instagram_data/\" + i[:-1] + \"/followers_and_following/following.json\"\n",
    "    p_priv = pth + \"instagram_data/\" + i[:-1] + \"/login_and_account_creation/account_privacy_changes.json\"\n",
    "    p_acc = pth + \"instagram_data/\" + i[:-1] + \"/account_information/personal_information.json\"\n",
    "\n",
    "    if not os.path.exists(Path(p_acc)):\n",
    "        continue\n",
    "    print(i)\n",
    "    temp = [i[:-1]]\n",
    "    if not os.path.exists(Path(p_likedposts)):\n",
    "        temp.append(0)\n",
    "    else:\n",
    "        with open(p_likedposts, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        data = pd.json_normalize(data, record_path =['likes_media_likes'])\n",
    "        temp.append(data.shape[0])\n",
    "\n",
    "    if not os.path.exists(Path(p_likedcomments)):\n",
    "        temp.append(0)\n",
    "    else:\n",
    "        with open(p_likedcomments, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        data = pd.json_normalize(data, record_path =['likes_comment_likes'])\n",
    "        temp.append(data.shape[0])\n",
    "\n",
    "    if not os.path.exists(Path(p_followers)):\n",
    "        temp.append(0)\n",
    "    else:\n",
    "        with open(p_followers, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        data = pd.json_normalize(data, record_path =['relationships_followers'])\n",
    "        temp.append(data.shape[0])\n",
    "\n",
    "    if not os.path.exists(Path(p_following)):\n",
    "        temp.append(0)\n",
    "    else:\n",
    "        with open(p_following, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        data = pd.json_normalize(data, record_path =['relationships_following'])\n",
    "        temp.append(data.shape[0])\n",
    "\n",
    "    with open(p_acc, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    data = pd.json_normalize(data, record_path =['profile_user'])\n",
    "    temp.append(data.iloc[0][-1])\n",
    "\n",
    "    temp.append([])\n",
    "    temp.append([])\n",
    "    if not os.path.exists(Path(p_priv)):\n",
    "        continue\n",
    "\n",
    "    with open(p_priv, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    data = pd.json_normalize(data, record_path =['account_history_account_privacy_history'])\n",
    "    data = data.set_axis(['title', 'href', 'value', 'timestamp'], axis=1, inplace=False)\n",
    "    df = data.groupby('title')['timestamp'].apply(list).reset_index(name='new')\n",
    "    temp2 = df['new'].tolist()\n",
    "    \n",
    "    if \"Switched to Public\" not in pd.unique(data['title']) and \"Switched to Private\" in pd.unique(data['title']):\n",
    "        temp[6] = temp2[0]\n",
    "    elif \"Switched to Private\" in pd.unique(data['title']):\n",
    "        temp[7] = temp2[1]\n",
    "        if \"Switched to Public\" in pd.unique(data['title']):\n",
    "            temp[6] = temp2[0]\n",
    "    saved_data.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['id', 'n_liked_post', 'n_liked_comment', 'n_follower', 'n_following', 'account_privacy', 'switch_to_public', 'switch_to_private']\n",
    "with open(\"./instagram_account_variable_data.csv\", 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow(fields)\n",
    "    write.writerows(saved_data)\n",
    "print(len(saved_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = []\n",
    "for i in lines:\n",
    "\n",
    "    p_likedposts = pth + \"instagram_data/\" + i[:-1] + \"/likes/liked_posts.json\"\n",
    "    p_likedcomments = pth + \"instagram_data/\" + i[:-1] + \"/likes/liked_comments.json\"\n",
    "    p_comments = pth + \"instagram_data/\" + i[:-1] + \"/comments/post_comments.json\"\n",
    "    p_posts = pth + \"instagram_data/\" + i[:-1] + \"/content/posts_1.json\"\n",
    "\n",
    "    print(i)\n",
    "    if not os.path.exists(Path(p_posts)):\n",
    "        continue\n",
    "    temp = [i[:-1]]\n",
    "    if not os.path.exists(Path(p_likedposts)):\n",
    "        temp.append(0)\n",
    "    else:\n",
    "        with open(p_likedposts, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        data = pd.json_normalize(data, record_path =['likes_media_likes'])\n",
    "        timelist = []\n",
    "        for ind, r in data.iterrows():\n",
    "            timelist.append(int(r[2][0]['timestamp']))\n",
    "        freq = 0\n",
    "        for ind in range(len(timelist)-1):\n",
    "            freq += timelist[ind] - timelist[ind+1]\n",
    "        freq = freq / len(timelist)-1\n",
    "        temp.append(freq)\n",
    "\n",
    "    if not os.path.exists(Path(p_likedcomments)):\n",
    "        temp.append(0)\n",
    "    else:\n",
    "        with open(p_likedcomments, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        data = pd.json_normalize(data, record_path =['likes_comment_likes'])\n",
    "        timelist = []\n",
    "        for ind, r in data.iterrows():\n",
    "            timelist.append(int(r[2][0]['timestamp']))\n",
    "        freq = 0\n",
    "        for ind in range(len(timelist)-1):\n",
    "            freq += timelist[ind] - timelist[ind+1]\n",
    "        freq = freq / len(timelist)-1\n",
    "        temp.append(freq)\n",
    "\n",
    "    if not os.path.exists(Path(p_comments)):\n",
    "        temp.append(0)\n",
    "    else:\n",
    "        with open(p_comments, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        data = pd.json_normalize(data, record_path =['comments_media_comments'])\n",
    "        timelist = []\n",
    "        for ind, r in data.iterrows():\n",
    "            timelist.append(int(r[2][0]['timestamp']))\n",
    "        freq = 0\n",
    "        for ind in range(len(timelist)-1):\n",
    "            freq += timelist[ind] - timelist[ind+1]\n",
    "        freq = freq / len(timelist)-1\n",
    "        temp.append(freq)\n",
    "\n",
    "    if not os.path.exists(Path(p_posts)):\n",
    "        temp.append(0)\n",
    "    else:\n",
    "        with open(p_posts, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        timelist = []\n",
    "        if isinstance(data, list):\n",
    "            for r in data:\n",
    "                timelist.append(int(r['media'][0]['creation_timestamp']))\n",
    "            freq = 0\n",
    "            for ind in range(len(timelist)-1):\n",
    "                freq += timelist[ind] - timelist[ind+1]\n",
    "            freq = freq / len(timelist)-1\n",
    "        else:\n",
    "            freq = data['creation_timestamp']\n",
    "        temp.append(freq)\n",
    "\n",
    "    saved_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fields = ['id', 'f_liked_post', 'f_liked_comment', 'f_comments', 'f_posts']\n",
    "with open(\"./instagram_account_activity_data.csv\", 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow(fields)\n",
    "    write.writerows(saved_data)\n",
    "print(len(saved_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "KEY = \"1d55848363814cf5b7a6c77b6f57250a\"\n",
    "\n",
    "ENDPOINT = \"https://lpfacapi.cognitiveservices.azure.com/\"\n",
    "\n",
    "IMAGE_BASE_URL = 'https://csdx.blob.core.windows.net/resources/Face/Images/'\n",
    "\n",
    "face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))\n",
    "\n",
    "print('-----------------------------')\n",
    "print()\n",
    "print('DETECT FACES')\n",
    "print()\n",
    "\n",
    "from PIL import Image\n",
    "#img = open('./252288878_271860704727291_8142963701425031159_n_17926857349794901.jpg', 'rb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Relavent features:\n",
    "    \"Number of faces detected: \", len(detected_faces)\n",
    "    for face in detected_faces: \n",
    "        print (face.face_id)\n",
    "        print (face.face_rectangle)\n",
    "        #print (face.face_landmarks)\n",
    "        print (face.face_attributes.blur)\n",
    "        print (face.face_attributes.emotion)\n",
    "        print (face.face_attributes.occlusion)\n",
    "        print (face.face_attributes.head_pose)\n",
    "        print (face.face_attributes.smile)\n",
    "        print (face.face_attributes.quality_for_recognition)\n",
    "    print()\"\"\"\n",
    "\n",
    "def detection(filename):\n",
    "    img = open(filename, 'rb')\n",
    "    detected_faces = face_client.face.detect_with_stream(img, return_face_rectangle = True, return_face_landmarks=True, return_face_attributes=[\"headPose\", \"smile\", \"emotion\", \"occlusion\", \"blur\", \"qualityForRecognition\"], detection_model='detection_01', recognition_model='recognition_04')\n",
    "    print(\"fdjhakljfla\", detected_faces)\n",
    "    return detected_faces\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = []\n",
    "n_face = []\n",
    "k = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = k\n",
    "#for i in lines:\n",
    "for t in range(m, len(lines)):\n",
    "\n",
    "    i = lines[t]\n",
    "    p_profilephoto = pth + \"instagram_data/\" + i[:-1] + \"/account_information/personal_information.json\"\n",
    "\n",
    "    if not os.path.exists(Path(p_profilephoto)):\n",
    "        continue\n",
    "    \n",
    "    print(i)\n",
    "    with open(p_profilephoto, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "    if  'Profile Photo' in data['profile_user'][0]['media_map_data'].keys():\n",
    "        p_profilephoto = pth + \"instagram_data/\" + i[:-1] + \"/\" + data['profile_user'][0]['media_map_data']['Profile Photo']['uri']\n",
    "    elif not data['profile_user'][0]['media_map_data']:\n",
    "        continue\n",
    "    elif '\\u00e5\\u00a4\\u00b4\\u00e5\\u0083\\u008f' in data['profile_user'][0]['media_map_data'].keys():\n",
    "        p_profilephoto = pth + \"instagram_data/\" + i[:-1] + '/' + data['profile_user'][0]['media_map_data']['\\u00e5\\u00a4\\u00b4\\u00e5\\u0083\\u008f']['uri']\n",
    "    else:\n",
    "        raise Exception('No face detected from image {}'.format(data['profile_user'][0]['media_map_data']))\n",
    "    \n",
    "    if not os.path.exists(Path(p_profilephoto)):\n",
    "        print(p_profilephoto)\n",
    "        continue\n",
    "\n",
    "    detected_faces = detection(p_profilephoto)\n",
    "\n",
    "    if not detected_faces:\n",
    "        saved_data.append([i[:-1]]+[0]*21)\n",
    "    else:\n",
    "        for face in detected_faces:\n",
    "            saved_data.append([i[:-1], face.face_rectangle.width, face.face_rectangle.height, \n",
    "                                face.face_rectangle.left, face.face_rectangle.top, face.face_attributes.blur.value, \n",
    "                                face.face_attributes.emotion.anger, face.face_attributes.emotion.contempt, face.face_attributes.emotion.disgust,\n",
    "                                face.face_attributes.emotion.fear, face.face_attributes.emotion.happiness, face.face_attributes.emotion.neutral,\n",
    "                                face.face_attributes.emotion.sadness, face.face_attributes.emotion.surprise, face.face_attributes.occlusion.forehead_occluded,\n",
    "                                face.face_attributes.occlusion.eye_occluded, face.face_attributes.occlusion.mouth_occluded,\n",
    "                                face.face_attributes.head_pose.roll, face.face_attributes.head_pose.yaw, face.face_attributes.head_pose.pitch,\n",
    "                                face.face_attributes.smile, face.face_attributes.quality_for_recognition])\n",
    "        n_face.append(len(detected_faces))\n",
    "    k += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fields = ['id', 'rec_width', 'rec_height', 'rec_left', 'rec_top', 'blur', 'emotion_anger', 'emotion_contempt', 'emotion_disgust', 'emotion_fear', 'emotion_happiness', 'emotion_neutral', 'emotion_sadness', 'emotion_surprise', 'forehead_occluded', 'eye_occluded', 'mouth_occluded', 'roll', 'yaw', 'pitch', 'smile', 'quality_for_recognition']\n",
    "with open(\"./instagram_profile_photo_data.csv\", 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow(fields)\n",
    "    write.writerows(saved_data)\n",
    "print(len(saved_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percent of face/no-face photo: \", len(n_face)/k)\n",
    "print(\"Average number of faces: \", sum(n_face)/len(n_face))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
